# üóÑÔ∏è Database Schema Architecture

**Version:** 2.0  
**Last Updated:** January 2026  
**Authors:** Yogendra Singh

---

## üìä Executive Summary

This document outlines the complete database architecture for the Nifty 50 Trading System, including data volume projections, schema design decisions, and implementation details.

**Key Decisions:**

- **Database**: TimescaleDB (PostgreSQL extension optimized for time-series)
- **Base Granularity**: **1-second tick data** (highest resolution)
- **Derived Views**: 1m, 5m, 15m, 1h, 1d, weekly (continuous aggregates)
- **Architecture**: One database with multiple logical schemas
- **Tables**: 22 tables across 6 domains
- **Storage Projection**:
  - **5 Years**: ~10-15 GB (with compression)
  - **10 Years**: ~20-30 GB (with compression)

---

## üìà Data Volume Analysis (1-Second Base)

### Trading Parameters

| Parameter          | Value                                     |
| ------------------ | ----------------------------------------- |
| Trading Hours      | 9:15 AM - 3:30 PM IST                     |
| Trading Duration   | 6h 15m = 375 minutes = **22,500 seconds** |
| Trading Days/Month | ~20 days                                  |
| Trading Days/Year  | ~240 days                                 |
| Stocks Tracked     | 50 (Nifty 50, expandable to 100+)         |

### Volume Projections (50 Stocks)

| Timeframe           | Rows/Day      | Rows/Month | Rows/Year | 5 Years          | 10 Years        |
| ------------------- | ------------- | ---------- | --------- | ---------------- | --------------- |
| **1-second (BASE)** | **1,125,000** | **22.5M**  | **270M**  | **1.35 Billion** | **2.7 Billion** |
| 1-minute (view)     | 18,750        | 375,000    | 4.5M      | 22.5M            | 45M             |
| 5-minute (view)     | 3,750         | 75,000     | 900,000   | 4.5M             | 9M              |
| 15-minute (view)    | 1,250         | 25,000     | 300,000   | 1.5M             | 3M              |
| 1-hour (view)       | 312           | 6,240      | 74,880    | 374,400          | 748,800         |
| 1-day (view)        | 50            | 1,000      | 12,000    | 60,000           | 120,000         |
| Weekly (view)       | 50            | 200        | 2,400     | 12,000           | 24,000          |

### Storage Estimates (1-Second Base Data)

| Duration     | Raw Size | With Compression (90%) | Notes                   |
| ------------ | -------- | ---------------------- | ----------------------- |
| **1 Year**   | ~20 GB   | **~2-3 GB**            | Production minimum      |
| **5 Years**  | ~100 GB  | **~10-15 GB**          | Recommended retention   |
| **10 Years** | ~200 GB  | **~20-30 GB**          | Full historical archive |

### Scalability Projections

| Stocks             | 5 Years (Compressed) | 10 Years (Compressed) | Notes                |
| ------------------ | -------------------- | --------------------- | -------------------- |
| 50 (Nifty 50)      | ~10-15 GB            | ~20-30 GB             | Current scope        |
| 100 (Nifty 100)    | ~20-30 GB            | ~40-60 GB             | 2x scaling           |
| 200 (Nifty 200)    | ~40-60 GB            | ~80-120 GB            | Enterprise scale     |
| 500 (FnO Universe) | ~100-150 GB          | ~200-300 GB           | Full market coverage |

### Storage Infrastructure Requirements

| Scale                 | Storage Type           | Recommended | Monthly Cost (AWS) |
| --------------------- | ---------------------- | ----------- | ------------------ |
| 50 stocks / 5 years   | SSD (GP3)              | 50 GB       | ~$5                |
| 50 stocks / 10 years  | SSD (GP3)              | 100 GB      | ~$10               |
| 200 stocks / 10 years | SSD (GP3)              | 500 GB      | ~$50               |
| 500 stocks / 10 years | SSD (GP3) + S3 tiering | 1 TB + Cold | ~$100              |

### Data Flow: Base ‚Üí Derived Views

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        1-SECOND TICKS (BASE TABLE)                           ‚îÇ
‚îÇ                     2.7 Billion rows / 10 years (~20-30 GB)                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                    ‚îÇ
                                    ‚ñº (Continuous Aggregates - Auto-computed)
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚ñº                           ‚ñº                           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  candles_1m   ‚îÇ           ‚îÇ  candles_5m   ‚îÇ           ‚îÇ  candles_15m  ‚îÇ
‚îÇ   45M rows    ‚îÇ           ‚îÇ    9M rows    ‚îÇ           ‚îÇ    3M rows    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ                           ‚îÇ                           ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                    ‚ñº
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚ñº                           ‚ñº                           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  candles_1h   ‚îÇ           ‚îÇ  candles_1d   ‚îÇ           ‚îÇ candles_weekly‚îÇ
‚îÇ   749K rows   ‚îÇ           ‚îÇ   120K rows   ‚îÇ           ‚îÇ   24K rows    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üèóÔ∏è Architecture Decision

### Single Database with Multiple Schemas

```
PostgreSQL/TimescaleDB (One Instance)
‚îÇ
‚îú‚îÄ‚îÄ Schema: market      ‚Üê Time-series hypertables
‚îÇ   ‚îî‚îÄ‚îÄ candles_1m, candles_5m/15m/1h/1d, options_chain
‚îÇ
‚îú‚îÄ‚îÄ Schema: reference   ‚Üê Master data (regular tables)
‚îÇ   ‚îî‚îÄ‚îÄ instruments, sectors, trading_calendar
‚îÇ
‚îú‚îÄ‚îÄ Schema: analysis    ‚Üê Computed data (hypertables)
‚îÇ   ‚îî‚îÄ‚îÄ signals, market_breadth, sector_strength, technical_indicators
‚îÇ
‚îú‚îÄ‚îÄ Schema: app         ‚Üê User data (regular tables)
‚îÇ   ‚îî‚îÄ‚îÄ users, user_alerts, user_watchlists, user_subscribers
‚îÇ
‚îú‚îÄ‚îÄ Schema: billing     ‚Üê Payment data (regular tables, isolate later)
‚îÇ   ‚îî‚îÄ‚îÄ plans, subscriptions, payments, invoices
‚îÇ
‚îî‚îÄ‚îÄ Schema: system      ‚Üê System metadata (mix)
    ‚îî‚îÄ‚îÄ data_availability, backfill_jobs, system_config, audit_log
```

### Why This Approach?

| Factor           | One DB + Schemas          | Multiple DBs          |
| ---------------- | ------------------------- | --------------------- |
| **Simplicity**   | ‚úÖ Easy to manage         | ‚ùå Complex            |
| **Cost**         | ‚úÖ Single instance        | ‚ùå Multiple instances |
| **JOINs**        | ‚úÖ Cross-domain queries   | ‚ùå Not possible       |
| **Scaling**      | ‚ö†Ô∏è Scale together         | ‚úÖ Independent        |
| **Future Split** | ‚úÖ Easy to separate later | N/A                   |

---

## üîÑ Data Watermark System (Duplicate Prevention)

### Problem

When running backfills, we need to:

1. Know what data already exists
2. Only fetch missing date ranges
3. Prevent duplicate insertions

### Solution: 3-Layer Protection

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  LAYER 1: data_availability Table                                ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  ‚îÇ
‚îÇ  Before fetching, check what data exists:                        ‚îÇ
‚îÇ  SELECT first_date, last_date FROM data_availability             ‚îÇ
‚îÇ  WHERE symbol = 'RELIANCE' AND timeframe = '1m';                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  LAYER 2: Unique Constraint                                      ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  ‚îÇ
‚îÇ  UNIQUE (time, symbol) on candles_1m                            ‚îÇ
‚îÇ  Database will reject duplicates automatically                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  LAYER 3: Upsert on Insert                                       ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  ‚îÇ
‚îÇ  INSERT INTO candles_1m (...) ON CONFLICT DO NOTHING;           ‚îÇ
‚îÇ  Silently skip any duplicates that slip through                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### data_availability Table Structure

```sql
CREATE TABLE data_availability (
    symbol      TEXT NOT NULL,
    timeframe   TEXT NOT NULL,        -- '1m', '5m', '1h', '1d'
    first_date  DATE NOT NULL,        -- Earliest data we have
    last_date   DATE NOT NULL,        -- Latest data we have
    total_rows  BIGINT DEFAULT 0,     -- Total records
    gaps        JSONB DEFAULT '[]',   -- Missing date ranges
    PRIMARY KEY (symbol, timeframe)
);
```

---

## üìã Complete Schema (22 Tables)

### 1. Market Data (6 entities)

| Table           | Type                 | Purpose             | Retention                       |
| --------------- | -------------------- | ------------------- | ------------------------------- |
| `candles_1m`    | Hypertable           | Base 1-minute OHLCV | 1 year hot, 10 years compressed |
| `candles_5m`    | Continuous Aggregate | Auto-rolled from 1m | Same as 1m                      |
| `candles_15m`   | Continuous Aggregate | Auto-rolled from 1m | Same as 1m                      |
| `candles_1h`    | Continuous Aggregate | Auto-rolled from 1m | Same as 1m                      |
| `candles_1d`    | Continuous Aggregate | Auto-rolled from 1m | Forever                         |
| `options_chain` | Hypertable           | Options Greeks/OI   | 30 days                         |

### 2. Reference Data (3 entities)

| Table              | Purpose                                          |
| ------------------ | ------------------------------------------------ |
| `instruments`      | Stock master (symbol, sector, token, is_nifty50) |
| `sectors`          | Sector definitions (IT, Banking, Auto, etc.)     |
| `trading_calendar` | Holidays, market hours                           |

### 3. Analysis Data (4 entities)

| Table                  | Type       | Purpose                    |
| ---------------------- | ---------- | -------------------------- |
| `signals`              | Hypertable | Generated buy/sell signals |
| `market_breadth`       | Hypertable | A/D ratio, new highs/lows  |
| `sector_strength`      | Hypertable | Sector rotation tracking   |
| `technical_indicators` | Hypertable | Cached RSI, EMA, MACD      |

### 4. User Data (4 entities)

| Table              | Purpose                                        |
| ------------------ | ---------------------------------------------- |
| `users`            | User accounts (telegram_id, email, is_premium) |
| `user_alerts`      | Price/indicator alerts                         |
| `user_watchlists`  | Custom stock lists                             |
| `user_subscribers` | Email newsletter subscriptions                 |

### 5. Billing Data (4 entities)

| Table           | Purpose                                               |
| --------------- | ----------------------------------------------------- |
| `plans`         | Subscription tiers (Free, Basic, Premium, Enterprise) |
| `subscriptions` | User subscription records                             |
| `payments`      | Payment transactions (Razorpay/Stripe)                |
| `invoices`      | Billing documents                                     |

### 6. System Data (4 entities)

| Table               | Purpose                              |
| ------------------- | ------------------------------------ |
| `data_availability` | Tracks what data exists (watermarks) |
| `backfill_jobs`     | Historical data job tracking         |
| `system_config`     | Key-value configuration store        |
| `audit_log`         | System event logging                 |

---

## üíæ Backup Strategy

### Backup Frequency

| Data Type    | Frequency | Retention | Method               |
| ------------ | --------- | --------- | -------------------- |
| Market Data  | Daily     | 30 days   | pg_dump ‚Üí S3         |
| User Data    | Hourly    | 7 days    | pg_dump ‚Üí S3         |
| Billing Data | Real-time | 1 year    | WAL archiving ‚Üí S3   |
| Configs      | On change | Forever   | Git (config as code) |

### Recovery Point Objective (RPO)

| Data Type    | Max Data Loss                       |
| ------------ | ----------------------------------- |
| Market Data  | 24 hours (can re-fetch from broker) |
| User Data    | 1 hour                              |
| Billing Data | 0 (real-time replication)           |

---

## üîß TimescaleDB Features Used

### 1. Hypertables

Auto-partitions data by time for efficient queries and management.

```sql
SELECT create_hypertable('candles_1m', 'time',
    chunk_time_interval => INTERVAL '1 day');
```

### 2. Continuous Aggregates

Automatic rollups from 1m ‚Üí 5m/15m/1h/1d.

```sql
CREATE MATERIALIZED VIEW candles_1h
WITH (timescaledb.continuous) AS
SELECT time_bucket('1 hour', time) AS time,
       symbol, first(open, time), max(high),
       min(low), last(close, time), sum(volume)
FROM candles_1m
GROUP BY 1, symbol;
```

### 3. Compression

90%+ space savings after data ages.

```sql
ALTER TABLE candles_1m SET (timescaledb.compress);
SELECT add_compression_policy('candles_1m', INTERVAL '7 days');
```

### 4. Retention Policies

Automatic cleanup of old data.

```sql
SELECT add_retention_policy('audit_log', INTERVAL '90 days');
```

---

## üìÅ Migration Files

| File                      | Description                                 |
| ------------------------- | ------------------------------------------- |
| `001_init_schema.sql`     | Original schema (candles, signals, options) |
| `002_extended_schema.sql` | Extended schema (users, billing, system)    |

### Apply Migration

```bash
docker exec -i timescaledb psql -U trading -d nifty50 \
  < layer-3-storage/timescaledb/migrations/002_extended_schema.sql
```

---

## üöÄ Query Examples

### Get latest prices for all Nifty 50

```sql
SELECT DISTINCT ON (symbol) symbol, time, close as price
FROM candles_1m
ORDER BY symbol, time DESC;
```

### Check data availability

```sql
SELECT symbol, first_date, last_date,
       (last_date - first_date) as days_available
FROM data_availability
WHERE timeframe = '1m'
ORDER BY symbol;
```

### Get daily OHLC

```sql
SELECT * FROM candles_1d
WHERE symbol = 'RELIANCE'
  AND time >= NOW() - INTERVAL '30 days'
ORDER BY time DESC;
```

---

## üìä Monitoring Queries

### Database size

```sql
SELECT pg_size_pretty(pg_database_size('nifty50'));
```

### Table sizes

```sql
SELECT hypertable_name,
       pg_size_pretty(hypertable_size(format('%I.%I', hypertable_schema, hypertable_name)))
FROM timescaledb_information.hypertables;
```

### Compression stats

```sql
SELECT hypertable_name,
       before_compression_total_bytes,
       after_compression_total_bytes,
       compression_ratio
FROM timescaledb_information.hypertable_compression_stats;
```

---

> **Next Steps:**
>
> 1. Seed `instruments` table with Nifty 50 stocks
> 2. Seed `trading_calendar` with 2026 holidays
> 3. Implement data watermark logic in ingestion layer

---

## ‚ùì FAQ - Architecture Discussion

_Questions raised by Yogendra Singh and Utkarsh during the design phase, with justifications._

---

### Q1: Should each Nifty 50 stock have its own table (50 tables) or one big table?

**Asked by:** Yogendra Singh

**Answer:** **One table for all stocks** ‚úÖ

**Justification:**

| Factor               | One Table                   | 50 Separate Tables                 |
| -------------------- | --------------------------- | ---------------------------------- |
| Query single stock   | ‚úÖ Fast with `symbol` index | ‚úÖ Fast                            |
| Query all 50 stocks  | ‚úÖ Single query             | ‚ùå Need 50 queries or UNION        |
| Compare stocks       | ‚úÖ Easy JOIN                | ‚ùå Complex cross-table JOINs       |
| Add new stock        | ‚úÖ Just insert data         | ‚ùå Create new table + code changes |
| Schema changes       | ‚úÖ Change 1 table           | ‚ùå Change 50 tables                |
| TimescaleDB features | ‚úÖ Full support             | ‚ö†Ô∏è Need 50 hypertables             |

**Evidence:** TimescaleDB's hypertable automatically partitions data by time. With a composite index on `(symbol, time DESC)`, queries for a single stock are just as fast as having separate tables, but with much simpler management.

---

### Q2: Should we use one database or multiple databases?

**Asked by:** Yogendra Singh

**Answer:** **One database with multiple schemas** ‚úÖ (for now)

**Justification:**

| Approach                | Pros                             | Cons                         |
| ----------------------- | -------------------------------- | ---------------------------- |
| **One DB**              | Simple, cheap, easy JOINs        | Scale together               |
| **Multiple DBs**        | Isolated scaling, security       | Complex, expensive, no JOINs |
| **One DB + Schemas** ‚úÖ | Best of both: organized + simple | Can split later if needed    |

**Evidence:** At current scale (50 stocks, ~300 MB/year), one database is sufficient. Schemas provide logical separation. If billing needs PCI-DSS compliance later, we can easily extract the `billing` schema to a separate database without redesigning the whole system.

---

### Q3: How do we track what data has been ingested and prevent duplicates?

**Asked by:** Yogendra Singh

**Answer:** **3-layer watermark system** ‚úÖ

**Justification:**

1. **`data_availability` table** - Check before fetching ‚Üí avoid redundant API calls
2. **Unique constraint** - Database rejects duplicates automatically
3. **`ON CONFLICT DO NOTHING`** - Silent skip for any edge cases

**Evidence:** Without this system:

- Backfill from Jan-Dec when Jun-Nov already exists = 12 months fetched
- With this system = only 5 months fetched (Jan-May)
- Saves **58% API calls** and prevents data corruption

---

### Q4: Should we use 1-second tick data as base or 1-minute candles?

**Asked by:** Yogendra Singh, Utkarsh

**Answer:** **1-second as base, with derived views for higher timeframes** ‚úÖ

**Justification:**

| Approach             | Pros                                    | Cons                            |
| -------------------- | --------------------------------------- | ------------------------------- |
| 1-minute base        | Smaller storage (~300 MB/10yr)          | Lose tick-level precision       |
| **1-second base** ‚úÖ | Full granularity, derive all timeframes | Larger storage (~20-30 GB/10yr) |

**Decision:** Store 1-second ticks, create continuous aggregates for 1m/5m/15m/1h/1d/weekly.

**Evidence:**

- **Storage is cheap**: 20-30 GB for 10 years costs ~$10/month on AWS
- **Flexibility**: Can always derive 1m from 1s, but cannot go back the other way
- **Future-proof**: If we need tick-level analysis (order flow, volume profile), data is available
- **Continuous aggregates**: TimescaleDB auto-computes higher timeframes with no extra code

**Derived Views (Auto-computed):**

```
1-second ‚Üí candles_1m ‚Üí candles_5m ‚Üí candles_15m ‚Üí candles_1h ‚Üí candles_1d ‚Üí candles_weekly
```

---

### Q5: What about storing user data, payments, and subscriptions?

**Asked by:** Yogendra Singh

**Answer:** **Include in same database, separate schema** ‚úÖ

**Justification:**

| Domain      | Volume                | Criticality           | Backup    |
| ----------- | --------------------- | --------------------- | --------- |
| Market data | High (45M rows/10yrs) | Medium (can re-fetch) | Daily     |
| User data   | Low (~10K users)      | High                  | Hourly    |
| Payments    | Low (~1K/month)       | **Critical**          | Real-time |

**Evidence:** With proper schemas:

- `app.users`, `app.user_alerts` ‚Üí User features
- `billing.payments`, `billing.invoices` ‚Üí Can migrate to separate DB for PCI-DSS later

We can JOIN user preferences with market data (e.g., "notify user X when RELIANCE crosses ‚Çπ2500") which isn't possible with separate databases.

---

### Q6: What database technology should we use?

**Asked by:** Yogendra Singh, Utkarsh

**Answer:** **TimescaleDB (PostgreSQL extension)** ‚úÖ

**Justification:**

| Requirement         | TimescaleDB              | Plain PostgreSQL | MongoDB      |
| ------------------- | ------------------------ | ---------------- | ------------ |
| Time-series queries | ‚úÖ Optimized             | ‚ö†Ô∏è Manual        | ‚ùå Not ideal |
| SQL support         | ‚úÖ Full                  | ‚úÖ Full          | ‚ùå No        |
| Compression         | ‚úÖ 90% built-in          | ‚ùå Manual        | ‚ö†Ô∏è Varies    |
| Auto-aggregates     | ‚úÖ Continuous aggregates | ‚ùå Must compute  | ‚ùå Manual    |
| Relational data     | ‚úÖ Yes (users, payments) | ‚úÖ Yes           | ‚ö†Ô∏è Limited   |

**Evidence:**

- Candle data is inherently time-series (timestamp + OHLCV)
- TimescaleDB auto-partitions by time (hypertables)
- Continuous aggregates auto-compute 5m/15m/1h/1d views
- Still PostgreSQL, so billing tables work normally

---

### Q7: How much storage and what are AWS cloud costs?

**Asked by:** Yogendra Singh

**Answer:** **~20-30 GB for 10 years, ~$15-25/month on AWS** ‚úÖ

**Storage Calculation (1-Second Base):**

```
1-second ticks:
  50 stocks √ó 22,500 ticks/day √ó 240 days/year √ó 10 years
  = 2,700,000,000 rows (2.7 Billion)

Row size: ~80 bytes (time, symbol, ltp, volume, bid, ask)
Raw size: 2.7B √ó 80 = ~216 GB

With TimescaleDB compression (90%): ~20-30 GB
```

**AWS Cloud Hosting Costs (Monthly):**

| Component           | Service             | Spec                      | Cost/Month     |
| ------------------- | ------------------- | ------------------------- | -------------- |
| **Database**        | RDS PostgreSQL      | db.t3.micro (1 vCPU, 1GB) | ~$15           |
| **Storage**         | GP3 SSD             | 100 GB                    | ~$10           |
| **Backup**          | Automated snapshots | 30 days retention         | ~$5            |
| **Redis Cache**     | ElastiCache         | cache.t3.micro            | ~$12           |
| **Total (Minimal)** |                     |                           | **~$42/month** |

**Cost by Scale:**

| Scale               | DB Instance  | Storage | Redis           | Total/Month |
| ------------------- | ------------ | ------- | --------------- | ----------- |
| **Dev/Test**        | db.t3.micro  | 50 GB   | cache.t3.micro  | **~$30**    |
| **50 stocks/5yr**   | db.t3.small  | 100 GB  | cache.t3.small  | **~$50**    |
| **50 stocks/10yr**  | db.t3.medium | 200 GB  | cache.t3.small  | **~$80**    |
| **200 stocks/10yr** | db.t3.large  | 500 GB  | cache.t3.medium | **~$150**   |
| **Enterprise**      | db.r6g.large | 1 TB    | cache.r6g.large | **~$400**   |

**Alternative: Local Docker (Current Setup):**

| Scenario                  | Storage    | Monthly Cost           |
| ------------------------- | ---------- | ---------------------- |
| Mac Mini M2 (home server) | 1 TB SSD   | ~$0 (electricity only) |
| EC2 Spot (t3.medium)      | 100 GB EBS | ~$15-20                |
| EC2 On-Demand (t3.medium) | 100 GB EBS | ~$35-40                |

**Evidence:** Current `docker-compose.prod.yml` runs entire stack (DB + Redis + Kafka + App) on t3.medium with 4GB RAM.

---

### Q8: How do we handle backup and recovery?

**Asked by:** Utkarsh

**Answer:** **Tiered backup strategy** ‚úÖ

| Data     | Backup        | Reason                             |
| -------- | ------------- | ---------------------------------- |
| Market   | Daily to S3   | Can re-fetch from broker if needed |
| Users    | Hourly to S3  | Critical, but low volume           |
| Payments | Real-time WAL | Zero data loss requirement         |
| Configs  | Git           | Version controlled                 |

**Evidence:** Market data is recoverable from broker APIs, so daily backups are sufficient. Payment data requires PCI-DSS compliance with point-in-time recovery, hence WAL archiving.

---

### Q9: Should we store computed indicators (RSI, EMA) in the database?

**Asked by:** Yogendra Singh

**Answer:** **Yes, cache in `technical_indicators` table** ‚úÖ

**Justification:**

| Approach           | Pros                    | Cons                |
| ------------------ | ----------------------- | ------------------- |
| Compute on-demand  | Always fresh            | Slow for 50 stocks  |
| **Cache in DB** ‚úÖ | Fast reads, precomputed | Slight staleness ok |

**Evidence:** Layer 4 (Analysis) computes RSI/EMA every minute. Storing in `technical_indicators` table means:

- Dashboard loads instantly (no recalculation)
- Historical analysis possible
- Can query "all stocks where RSI < 30"

---

_Document prepared during architecture planning session, January 2026_
