/**
 * Layer 1: Data Ingestion Service
 *
 * Connects to market data sources via WebSocket,
 * normalizes data, and publishes to Kafka
 *
 * @author Yogendra Singh
 */

// Register ts-node to handle TypeScript files in node_modules regarding the SDK
require('ts-node').register({
  transpileOnly: true,
  ignore: [/node_modules\/(?!@mstock-mirae-asset)/],
});

require('dotenv').config(); // Load local .env if exists
require('dotenv').config({ path: '../.env' }); // Also load root .env for local development

// IMPORTANT: Initialize global axios interceptor BEFORE importing SDKs
// This patches axios globally to track all external API calls
const { setupGlobalInterceptor } = require('./utils/axios-interceptor');
setupGlobalInterceptor();

const path = require('path');
const { fork } = require('child_process');

const express = require('express');
const { VendorFactory } = require('./vendors/factory');
const { KafkaProducer } = require('./kafka/producer');
const { Normalizer } = require('./normalizer');
const { logger } = require('./utils/logger');
const { metrics, register } = require('./utils/metrics');
const client = require('prom-client');
const symbols = require('../config/symbols.json');

// Initialize Express for health checks
const app = express();
const PORT = process.env.INGESTION_PORT || 3001;

const httpRequestDurationMicroseconds = new client.Histogram({
  name: 'http_request_duration_seconds',
  help: 'Duration of HTTP requests in seconds',
  labelNames: ['method', 'route', 'code'],
  buckets: [0.1, 0.3, 0.5, 0.7, 1, 3, 5],
  registers: [register],
});

// Use shared metrics instead of local ones
const ticksReceivedCounter = metrics.ticksCounter;
const kafkaMessagesSentCounter = metrics.kafkaMessagesSent;
const websocketConnectionGauge = metrics.websocketConnections;

// Middleware for HTTP request duration
app.use((req, res, next) => {
  const end = httpRequestDurationMicroseconds.startTimer();
  res.on('finish', () => {
    end({ method: req.method, route: req.route ? req.route.path : req.path, code: res.statusCode });
  });
  next();
});

// Metrics Endpoint
app.get('/metrics', async (req, res) => {
  try {
    res.set('Content-Type', register.contentType);
    res.end(await register.metrics());
  } catch (ex) {
    res.status(500).end(ex);
  }
});

// Initialize components
let marketDataVendor;
let kafkaProducer;
let normalizer;

/**
 * Initialize all services
 */
async function initialize() {
  logger.info('üöÄ Starting Layer 1: Data Ingestion Service');

  try {
    // Initialize Kafka Producer
    kafkaProducer = new KafkaProducer({
      brokers: process.env.KAFKA_BROKERS?.split(',') || ['localhost:9092'],
      topic: process.env.KAFKA_TOPIC_RAW_TICKS || 'raw-ticks',
    });
    await kafkaProducer.connect();
    logger.info('‚úÖ Kafka Producer connected');

    // Initialize Redis for System Metrics
    const redis = require('redis');
    const redisClient = redis.createClient({
      url: process.env.REDIS_URL || 'redis://localhost:6379',
    });
    redisClient.on('error', (err) => logger.error('Redis Client Error', err));
    await redisClient.connect();
    logger.info('‚úÖ Redis Metric Publisher connected');

    const logToRedis = async (message) => {
      try {
        const timestamp = new Date().toLocaleTimeString();
        const logEntry = `[${timestamp}] ${message}`;
        await redisClient.lPush('system:layer1:logs', logEntry);
        await redisClient.lTrim('system:layer1:logs', 0, 49);
      } catch (e) {
        logger.error('Failed to log to Redis', e);
      }
    };

    await logToRedis('üöÄ Layer 1 Ingestion Service Started');

    // Start Metrics Publishing Loop
    setInterval(async () => {
      try {
        const mem = process.memoryUsage();
        const packetsVal = await metrics.websocketPackets.get();
        const bytesVal = await metrics.websocketDataBytes.get();
        const ticksVal = await metrics.ticksCounter.get();

        const l1Metrics = {
          heap_used: (mem.heapUsed / 1024 / 1024).toFixed(2) + 'MB',
          uptime: process.uptime().toFixed(0) + 's',
          websocket_packets: (packetsVal?.values[0]?.value || 0).toLocaleString(),
          websocket_data_kb: ((bytesVal?.values[0]?.value || 0) / 1024).toFixed(2) + ' KB',
          type: 'Stream',
          source: 'MStock',
          timestamp: Date.now(),
        };
        await redisClient.set('system:layer1:metrics', JSON.stringify(l1Metrics));

        // Periodically log tick summary to Redis
        const totalTicks = ticksVal?.values.reduce((sum, v) => sum + v.value, 0) || 0;
        if (totalTicks > 0) {
          await logToRedis(
            `üìä Ingestion Health: ${totalTicks.toLocaleString()} total ticks received`
          );
        }
      } catch (e) {
        logger.error('Metric Publish Error', e);
      }
    }, 10000); // 10 seconds

    // Initialize Normalizer
    normalizer = new Normalizer();
    logger.info('‚úÖ Normalizer initialized');
    await logToRedis('‚úÖ Normalizer initialized');

    // Load Subscription List from Global Shared Map
    let subscriptionList = [];
    try {
      // In Docker: /app/src/index.js -> ../vendor = /app/vendor (mounted)
      // Locally: layer-1-ingestion/src/index.js -> ../vendor = layer-1-ingestion/vendor (symlink)
      const mapPath = path.resolve(__dirname, '../vendor/nifty50_shared.json');
      const masterMap = require(mapPath);

      // Map to MStock Format: "NSE:Token"
      // Filter out items without mstock token
      subscriptionList = masterMap
        .filter((item) => item.tokens && item.tokens.mstock)
        .map((item) => `NSE:${item.tokens.mstock}`);

      logger.info(`üìã Loaded ${subscriptionList.length} Symbols from Global Map for Subscription.`);
    } catch (e) {
      logger.warn(
        `‚ö†Ô∏è Failed to load Global Map: ${e.message}. Falling back to config/symbols.json (Legacy)`
      );
      // Fallback or Empty
      subscriptionList = symbols.nifty50.map((s) => `NSE:${s.token}`); // Legacy fallback (might be Kite tokens!)
    }

    // Initialize Market Hours (MUST be before VendorManager for status checks)
    const { MarketHours } = require('./utils/market-hours');
    const marketHours = new MarketHours();

    // Initialize Market Data Vendor via Factory
    const { VendorManager } = require('./vendors/manager');
    marketDataVendor = new VendorManager({
      // apiKey: process.env.ZERODHA_API_KEY, // Passed via Env to factory
      symbols: subscriptionList,
      onTick: handleTick,
    });

    marketDataVendor.init();
    await marketDataVendor.connect();

    const marketStatus = marketHours.isMarketOpen();
    const statusText = marketStatus ? 'Market is OPEN - Stream active' : 'Market is CLOSED - Idle';
    await logToRedis(`üì° Connected to MStock. ${statusText}`);
    logger.info(`üéØ Subscribed to ${subscriptionList.length} Nifty 50 symbols (Stream Mode)`);

    const runScriptWithIPC = (scriptPath, args = []) => {
      return new Promise((resolve, reject) => {
        const child = fork(scriptPath, args, { stdio: 'inherit' });

        child.on('message', (msg) => {
          if (msg.type === 'metric') {
            try {
              // Direct lookup by key in the metrics object
              const metric = metrics[msg.name];
              if (metric) {
                logger.debug(
                  `üìà IPC Metric Update: ${msg.name} | ${JSON.stringify(msg.labels)} | +${msg.value || 1}`
                );
                if (metric.inc) metric.inc(msg.labels, msg.value || 1);
                else if (metric.set) metric.set(msg.labels, msg.value || 1);
                else if (metric.observe) metric.observe(msg.labels, msg.value || 1);
              } else {
                logger.warn(
                  `‚ö†Ô∏è Metric ${msg.name} not found in parent. Available: ${Object.keys(metrics).join(', ')}`
                );
              }
            } catch (err) {
              logger.error(`Failed to update metric ${msg.name} from child`, err);
            }
          }
        });

        child.on('close', (code) => {
          if (code === 0) resolve();
          else reject(new Error(`Exit code ${code}`));
        });
      });
    };

    let isBackfilling = false;

    const updateBackfillStatus = async (status, progress = 0, details = '') => {
      try {
        const statusObj = {
          status, // 0:Idle, 1:Run, 2:Done, 3:Fail
          progress,
          details,
          job_type: 'historical_backfill',
          timestamp: Date.now(),
        };
        await redisClient.set('system:layer1:backfill', JSON.stringify(statusObj));

        // Update Prometheus
        if (status === 1 || status === 'running') {
          metrics.batchJobStatus.set({ job_type: 'historical_backfill' }, 1);
        } else if (status === 2 || status === 'completed') {
          metrics.batchJobStatus.set({ job_type: 'historical_backfill' }, 2);
        } else if (status === 3 || status === 'failed') {
          metrics.batchJobStatus.set({ job_type: 'historical_backfill' }, 3);
        } else {
          metrics.batchJobStatus.set({ job_type: 'historical_backfill' }, 0);
        }

        if (progress > 0) {
          metrics.batchJobProgress.set(
            { job_type: 'historical_backfill', metric: 'percent' },
            progress
          );
        }
      } catch (e) {
        logger.error('Failed to update backfill status', e);
      }
    };

    const runBackfill = async (startParams = {}) => {
      if (isBackfilling) {
        logger.warn('‚ö†Ô∏è Backfill already in progress. Skipping...');
        return;
      }
      isBackfilling = true;
      try {
        const startTime = Date.now();
        const { fromDate, toDate, symbol } = startParams;
        const symbolMsg = symbol ? `Symbol: ${symbol}` : 'All Symbols';
        const dateMsg = fromDate && toDate ? `(${fromDate} to ${toDate})` : '(Last 5 Days)';

        await updateBackfillStatus(1, 5, `Starting Backfill... ${symbolMsg} ${dateMsg}`);

        // 1. Run Batch Fetch
        logger.info(`‚è≥ Step 1: Fetching Historical Data... ${symbolMsg} ${dateMsg}`);
        await updateBackfillStatus(1, 10, `Fetching Data... ${symbolMsg}`);

        const batchScript = path.resolve(__dirname, '../scripts/batch_nifty50.js');

        // Construct Arguments based on params
        const scriptArgs = [];
        if (symbol) {
          scriptArgs.push('--symbol', symbol);
        }
        if (fromDate && toDate) {
          scriptArgs.push('--from', fromDate, '--to', toDate);
        } else {
          scriptArgs.push('--days', '5');
        }

        await runScriptWithIPC(batchScript, scriptArgs);

        await updateBackfillStatus(1, 50, 'Step 1 Complete: Data Downloaded');

        // 2. Feed Kafka
        logger.info('‚è≥ Step 2: Feeding Data to Kafka...');
        await updateBackfillStatus(1, 55, 'Feeding Data to Kafka...');
        const feedScript = path.resolve(__dirname, '../scripts/feed_kafka.js');
        await runScriptWithIPC(feedScript);

        const duration = (Date.now() - startTime) / 1000;
        metrics.batchJobDuration.observe(
          { job_type: 'historical_backfill', status: 'success' },
          duration
        );

        await updateBackfillStatus(2, 100, 'Backfill Complete');
        logger.info('‚úÖ Backfill Complete.');
      } catch (err) {
        await updateBackfillStatus(3, 0, err.message);
        logger.error(`‚ùå Backfill Failed: ${err.message}`);
      } finally {
        isBackfilling = false;
      }
    };

    // --- Command Listener (Always Active) ---
    try {
      const commandClient = redis.createClient({
        url: process.env.REDIS_URL || 'redis://localhost:6379',
      });
      await commandClient.connect();
      await commandClient.subscribe('system:commands', async (message) => {
        try {
          const { command, params } = JSON.parse(message);
          if (command === 'START_BACKFILL') {
            logger.info(
              `üì• Received START_BACKFILL command. Triggering backfill with params: ${JSON.stringify(params)}`
            );
            runBackfill(params);
          }
        } catch (e) {
          logger.error('Command Parse Error', e);
        }
      });
      logger.info('‚úÖ Command Listener connected');
    } catch (e) {
      logger.error('‚ùå Failed to connect Command Listener:', e);
    }

    // --- Auto-Run if Market is Closed ---
    if (!marketHours.isMarketOpen()) {
      logger.info('üåô Market is Closed. Auto-triggering Historical Data Backfill...');
      runBackfill();
    }
  } catch (error) {
    logger.error('‚ùå Initialization failed:', error);
    process.exit(1);
  }
}

/**
 * Handle incoming tick data
 * @param {Object} tick - Raw tick from broker
 */
async function handleTick(tick) {
  try {
    // Normalize the tick to unified schema
    const normalizedTick = normalizer.normalize(tick);

    if (!normalizedTick) {
      metrics.invalidTicksCounter.inc();
      return;
    }

    // Publish to Kafka (partitioned by symbol)
    await kafkaProducer.send(normalizedTick);

    // Update metrics
    metrics.ticksCounter.inc({ symbol: normalizedTick.symbol });
    metrics.ticksPerSecond.inc();
  } catch (error) {
    logger.error('Error processing tick:', error);
    metrics.errorCounter.inc({ type: 'tick_processing' });
  }
}

// Health check endpoint
app.get('/health', (req, res) => {
  const health = {
    status: 'healthy',
    service: 'layer-1-ingestion',
    timestamp: new Date().toISOString(),
    connections: {
      kafka: kafkaProducer?.isConnected() || false,
      websocket: marketDataVendor?.isConnected() || false,
    },
  };
  res.json(health);
});

// Metrics endpoint for Prometheus
app.get('/metrics', async (req, res) => {
  res.set('Content-Type', register.contentType);
  res.end(await register.metrics());
});

// Graceful shutdown
async function shutdown() {
  logger.info('üõë Shutting down gracefully...');

  if (marketDataVendor) await marketDataVendor.disconnect();
  if (kafkaProducer) await kafkaProducer.disconnect();

  process.exit(0);
}

process.on('SIGTERM', shutdown);
process.on('SIGINT', shutdown);

// Start the service
app.listen(PORT, async () => {
  logger.info(`üì° Health check server running on port ${PORT}`);

  // Interactive Mode Check (Local Dev Only)
  if (process.stdin.isTTY) {
    await checkInteractiveMode();
  } else {
    initialize();
  }
});

/**
 * Check if user wants running interactive historical fetch
 */
async function checkInteractiveMode() {
  const readline = require('readline');
  const path = require('path');

  // Load Global Master Map (Local Dev Path)
  let masterMap = [];
  try {
    const mapPath = path.resolve(__dirname, '../../vendor/nifty50_shared.json');
    masterMap = require(mapPath);
    // console.log(`Loaded Master Map from ${mapPath}`);
  } catch (e) {
    console.warn(
      '‚ö†Ô∏è Could not load Global Master Map (vendor/nifty50_shared.json). Symbol resolution disabled.'
    );
  }

  const rl = readline.createInterface({
    input: process.stdin,
    output: process.stdout,
  });

  const ask = (question, def) =>
    new Promise((resolve) => {
      rl.question(`${question} (Default: ${def}): `, (answer) => {
        resolve(answer.trim() || def);
      });
    });

  try {
    console.log('\n==========================================');
    console.log('   üöÄ LAYER 1 INGESTION: INTERACTIVE MODE   ');
    console.log('==========================================\n');

    // Timeout check to default to normal mode if no input
    let answered = false;
    const timer = setTimeout(() => {
      if (!answered) {
        console.log('\n‚è≥ Timeout: Starting standard ingestion mode...');
        rl.close();
        initialize();
      }
    }, 5000);

    const mode = await ask('Run Historical Fetch Job? (y/N)', 'N');
    answered = true;
    clearTimeout(timer);

    if (mode.toLowerCase() === 'y') {
      console.log('\n--- Configuration ---');
      const exchange = await ask('Exchange', 'NSE');

      // Ask for Symbol or Token
      let symbolInput = await ask('Symbol Name (e.g. RELIANCE) or Token', '22');
      let symbolToken = symbolInput;
      let resolvedName = '';

      // Try to resolve symbol to MStock Token using Master Map
      const match = masterMap.find((s) =>
        s.symbol.toUpperCase().includes(symbolInput.toUpperCase())
      );

      if (match && match.tokens && match.tokens.mstock) {
        symbolToken = match.tokens.mstock;
        resolvedName = match.symbol;
        console.log(
          `‚úÖ Resolved '${symbolInput}' to MStock Token: ${symbolToken} (${resolvedName})`
        );
      } else {
        console.log(`‚ÑπÔ∏è Using raw input '${symbolInput}' as token (No match in Master Map).`);
      }

      const interval = await ask('Interval', 'TEN_MINUTE');

      const { DateTime } = require('luxon');
      const defFrom = DateTime.now().minus({ days: 3 }).toFormat('yyyy-MM-dd HH:mm:ss');
      const defTo = DateTime.now().toFormat('yyyy-MM-dd HH:mm:ss');

      const fromDate = await ask('From Date', defFrom);
      const toDate = await ask('To Date', defTo);

      rl.close();
      await runInteractiveJob({ exchange, symbolToken, interval, fromDate, toDate });
    } else {
      rl.close();
      initialize();
    }
  } catch (e) {
    console.error('Interactive Error:', e);
    initialize(); // Fallback
  }
}

/**
 * Run manual fetch job based on interactive input
 */
async function runInteractiveJob(config) {
  logger.info(`üõ† Running Manual Job: ${JSON.stringify(config)}`);

  // Init minimal components
  try {
    const kafkaProducer = new KafkaProducer({
      brokers: process.env.KAFKA_BROKERS?.split(',') || ['localhost:9092'],
      topic: process.env.KAFKA_TOPIC_RAW_TICKS || 'raw-ticks',
    });
    await kafkaProducer.connect();

    const normalizer = new Normalizer();

    const vendor = VendorFactory.createVendor({
      // Pass minimal options. Warning: MStock might expect symbols to be array
      symbols: [`${config.exchange}:${config.symbolToken}`],
      onTick: () => {}, // Logic is pull-based here
    });

    await vendor.connect();

    const params = {
      exchange: config.exchange,
      symboltoken: config.symbolToken,
      interval: config.interval,
      fromdate: config.fromDate,
      todate: config.toDate,
    };

    const response = await vendor.fetchData(params);

    if (response.status && response.data && Array.isArray(response.data.candles)) {
      const candles = response.data.candles;
      logger.info(`‚úÖ Received ${candles.length} candles.`);

      for (const candle of candles) {
        // Mock tick structure for normalizer?
        // Usually Historical Data goes to a different topic or handled differently.
        // But for "Ingestion", we might want to normalize and push?
        // Or just log as per user verification request.
        // Given user context: "feed tis input ... like reliace last 10 days"
        // I will Log to console and Option to Publish
        logger.info(`Candle: ${JSON.stringify(candle)}`);
      }
    } else {
      logger.error(`‚ùå Fetch Failed: ${response.message}`);
    }

    await vendor.disconnect();
    await kafkaProducer.disconnect();
    process.exit(0);
  } catch (e) {
    logger.error(`Job Failed: ${e.message}`);
    process.exit(1);
  }
}
